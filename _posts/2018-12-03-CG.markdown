---
layout: post
title:  A Brief Primer On Efficient Rendering Algorithms & Clustered Shading.
date:   2018-12-02 23:13:40
description: An introduction to basic efficient rendering concepts and an overview of my Clustered Shading implementation. 
---

### **Table of Contents**
---

* [**Part 1: Efficient Rendering Algorithm**](#part-1)
    * [Forward Shading](#forward-shading)
    * [Deferred Shading](#deferred-shading)
    * [Tiled Shading & Forward+](#tiled-shading--forward)
    * [Clustered Shading](#clustered-shading)

* [**Part 2: Clustered Hybrid Shading in Depth**](#part-2)
    * [Building a Cluster Grid](#building-a-cluster-grid)
    * [Determining Active Clusters](#determining-active-clusters)
    * [Light Culling Methods](#light-culling-methods)
    * [Optimization Techniques](#optimization-techniques)
    * [Successful Implementations](#successful-implementations)

---
<p></p>
For the past three months Iâ€™ve been working on a Clustered Renderer in OpenGL with the aim of building a framework to experiment with deferred and forward shading techniques. Feel free to [check it out on GitHub.](https://github.com/Angelo1211/HybridRenderingEngine) I think it's pretty neat, but I also might be a bit biased. Originally my plan for this post was to write a brief account detailing how I implemented clustered rendering in my engine. However, as the writing went on I increasingly found myself wanting to refer to other efficient algorithms as a way to contextualize the benefits of Clustered Shading. Those references quickly grew into a significant portion of the text, so I decided to split it into two parts.

Part one is a high-level overview of efficient rendering techniques. It follows a simple three-act structure that will take us from classical forward shading all the way to a modern implementation of Clustered Shading. First, I'll discuss how a given algorithm renders an image. Next, I'll showcase its major shortcomings and limitations. And lastly, I'll introduce strategies to overcome those shortcomings, which will segue nicely to the next algorithm. Part two is a deep dive into my own flavor of Clustered Shading, covering all major steps of the algorithm and laying out the specifics of the code I wrote. It'll also contain some links to further readings on the topic written by developers who have succesfully implemented this technique in high profile games. Let's get to it!

### **TL;DR**
---
<p></p>
> *Clustered shading is an efficient and versatile rendering algorithm capable of using both forward and deferred shading systems. It divides the view frustum into a 3D grid of blocks or "clusters" and quickly computes a list of lights intersecting each active volume.*

---
*You can fit this in a tweet!*

<p></p>

# ***Part 1:***
<p></p>
## Efficient Rendering Algorithms
<p></p>
---
*...versatile **rendering algorithm** capable...*
*...an **efficient** and...*
*...a **list of lights** intersecting...*
*...both **forward and deferred shading systems**...*
*...the **view frustum** into a 3D grid ...

 The field of Real-Time rendering has two major opposing driving forces. On the one hand, there's the strive for realism and a wish for ever improving image quality. On the other hand, there is a hard constraint to deliver those images quickly, usually in less than a 1/30th of a second. The only way to bridge this gap is through the use of software optimizations that allow you to squeeze as much performance as possible from the underlying hardware.
 
Without these optimizations, it would be impossible to achieve the levels of visual fidelity we are accustomed to in modern high-profile games. So, in a sense, **efficient rendering algorithms** are at the core of all modern Real-Time rendering processes and are needed to display nearly anything these days. However, of the countless possible optimizations that one could implement, there are a few notable techniques that can have a dramatic impact on performance and have been fully embraced by the graphics community.

We'll be focusing on two kinds of techniques: those that reduce the amount of unnecessary pixel shader calls and others that focus on lowering the number of lighting computations during shading. You should assume that for most of the algorithms we'll be discussing all other bottlenecks have already been taken care of. Also, proper culling and mesh management have been performed somewhere further upstream. Other important details that come into play when rendering a mesh will have also been stripped away in favor of presenting a clearer picture of how and why these efficient algorithms outperform their predecessors.

However, there ain't no such thing as a free lunch. None of these algorithms have zero overhead or are guaranteed to improve the performance of your renderingangel@angel-MS-7971 ~ $ emacs
 pipeline. So make sure to consult your ~~[primary care phycisian](https://renderdoc.org/)~~ profiler tools to assess the location of your current bottleneck. 

Let's begin by examining the most basic rendering algorithm: 

### **Forward Shading**
<p></p>
<script>
    $( document ).ready( function(){
        var cw = $('#forward1').width() * 0.80 ;
        $('#forward1').css({'height':cw  +'px'});
        $('#overdraw1').css({'height':cw  +'px'});
        $('#zpre1').css({'height':cw  +'px'});
        $('#def1').css({'height':cw  +'px'});
        $('#tiled1').css({'height':cw  +'px'});
        $('#depthDisc1').css({'height':cw * 2.1  +'px'});
        $('#clus1').css({'height':(cw * 2.1) +'px'});
        $('#clus2').css({'height':(cw * 2.1) +'px'});
        $('#zslice1').css({'height':cw  +'px'});
        $('#zslice2').css({'height':cw  +'px'});
        $('#video1').css({'height':cw  +'px'});
    });
</script>
 
<iframe id="forward1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/forward.html" style = "width: 100%;" frameborder="0" scrolling="no"  ></iframe>

{% highlight c# %}
//Shaders:
Shader simpleShader

//Buffers:
Buffer display

for mesh in scene
    for light in scene
        display += simpleShader(mesh, light)
{% endhighlight %} 

In it's vanilla form, **forward shading** is a method of rendering scenes by linearly marching *forward* along the GPU pipeline. A single shader program will be completely in charge of drawing a mesh, starting from the vertex shader, passing through to the rasterizer and finally getting to the fragment shader. The animation and pseudocode above illustrate this behaviour at a glance. For each mesh and light combination we issue a single draw call additively blending the results until the image has been fully assembled. This process is better known as **multi-pass** forward rendering.

This simple approach leaves a lot of room for improvement, but I wager that it would probably fair fine for simple scenes with a few meshes, basic materials and just a couple of lights. The main issues with this algorithm will become apparent when you increase the quantity or complexity of any of these variables.

For example, a scene with many dynamic lights of different kinds will overwhelm any multi-pass forward renderer due to the combinatorial growth in the number of shader programs. Even though each individual shader is simple, the overhead from shader program switching and bandwidth costs of repeatedly loading meshes becomes a bottleneck quickly. In addition, as you increase the number of meshes you will run into an issue called **overdraw**. You can see it in action in the animation below.

<iframe id="overdraw1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/overdraw.html" style = "width:100%" frameborder="0" scrolling="no"  ></iframe>

We begin with the completed scene from the previous animation and then draw new meshes that overlap the previous ones. This invalidates all of the work done shading the covered parts of the background objects. In multi-pass forward rendering this is especially costly, since each completed pixel went through the full vertex-fragment shader pipeline multiple times before being discarded. Alternatively, we could sort the meshes front to back before rendering so that the fragment depth check takes care of the problem. However, sorting meshes based on their screen location is not a trivial problem to solve for large scenes and is costly to do every frame. 

What we need instead is a solution that decouples the process of determining if a mesh contributes to the final image from the act of shading a given mesh.

### **Deferred Shading**
<p></p>

**Deferred shading** is that solution. The idea is that we perform all visibility and material fetching in one shader program, store the result in a buffer, and then we *defer* lighting and shading to another shader that takes as an input that buffer. As Angelo Pesce discusses in [The real-time rendering continuum: a taxonomy](http://c0de517e.blogspot.com/2016/08/the-real-time-rendering-continuum.html) there is a lot of freedom as to where we can choose to perform this split. However, the size and content of the intermediate buffer will be determined by the splitting point.

If we're only concerned with overdraw, the buffer simply has to keep track of the last visible mesh per pixel, normally by recording either its depth or position. Before shading we check if each pixel in a mesh matches the corresponding value recorded in the buffer, if not we discard it. The buffer will be initialized to the far plane depth, that way any mesh that is visible, and therefore closer, will be recorded. To summarize using pseudocode:
 
<div>
<iframe id="zpre1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/zprepass.html" style = "width:100%" frameborder="0" scrolling="no"  ></iframe>
</div>

{% highlight c# %}
//Buffers:
Buffer display
Buffer depthBuffer

//Shaders:
Shader simpleShader
Shader writeDepth

//Visibility
for mesh in scene
    if mesh.depth < depthBuffer.depth
       depthBuffer = writeDepth(mesh)

//Shading and lighting
for mesh in scene
    if mesh.depth == depthBuffer.depth
        for light in scene
            display += simpleShader(mesh, light)
{% endhighlight %}

This is commonly known as a **Z Pre-pass**, and it is considered a *mild* decoupling of shading and visibility for the sake of reducing the impact of overdraw. And I say *reducing* instead of *eliminating*, because overdraw still occurs when you are populating the depth buffer. However, at this point its impact on performance is minimal. This will only hold true if generating geometry and meshes are somewhat straightforward processes in your engine. If meshes come from procedural generation, heavy tessellation or some other source, loading them a second time might negate any possible benefits.

Usually, mentions of **Deferred shading** refer to a specific system that uses a larger buffer. Within this buffer a series of textures are stored that include information about not only depth, but other shading attributes too. How a deferred system is implemented varies from renderer to renderer, albeit it usually includes material data such as: surface normal, albedo/color or a specular/shininess component. The collection of these textures is commonly referred to as the **G-Buffer**. If you want to see a specific implementation [this Graphics Study](http://www.adriancourreges.com/blog/2017/12/15/mgs-v-graphics-study/) by  Adrian Courreges goes into detail about how a frame is rendered in Metal Gear Solid V, a fantastic game with a a fully deferred rendering engine.

Let's update our pseudocode and check out how a basic deferred renderer is implemented:

<iframe id="def1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/deferred.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

{% highlight c# %}
//Buffers:
Buffer display
Buffer GBuffer 

//Shaders:
Shader simpleShader
Shader writeShadingAttributes

//Visibility & materials
for mesh in scene
    if mesh.depth < GBuffer.depth
       GBuffer = writeShadingAttributes(mesh)

//Shading & lighting - Multi-pass
for light in scene
    display += simpleShader(GBuffer, light)
{% endhighlight %}

Our G-Buffer contains the following shading attributes presented above in left to right, top to bottom  order: surface roughness, normals, albedo and world space position. Notice the differences in material properties between the ico-sphere, the regular sphere and the cube. Lighting is now evaluated for the whole G-Buffer at once instead of per mesh, but still done in a multi-pass fashion.

Deferred shading can also be understood as a tool that allows us to trade off memory bandwidth in exchange for a reduction in computation. In the case of multi-pass setups, the G-Buffer reduces the costs of shading because it can cache vertex transformation results and interpolated shaded attributes, which we later reuse each lighting pass by just reading the G-Buffer.

Yet, reading and writing memory is not problem free either. Due to latency, all memory requests take time to be serviced. Even though GPU's can hide this latency by working on some other tasks while waiting, if the ratio of compute operations to memory reads in a program is very low it will eventually have no other option but to stall and wait until the memory delivers on the request. No amount of latency hiding can deal with a program that is requesting data at too high a rate. If you're interested in understanding why [Prof. Fatahalian's course on Paralallel Computer architectures](http://15418.courses.cs.cmu.edu/tsinghua2017/lecture/basicarch) covers the modern Multi-core processor architecture, both for CPU's and GPU's, and explains memory latency and throughput oriented systems in detail. 

> #### Small Detour: How slow are texture reads anyway?
> ---
> Texture reads are essentially memory reads and reading memory **is always slow.** How slow you ask? Well, there is no exact answer since it varies from architecture to architecture, but it is generally considered to be about ~200x slower than a register operation, which we can assume to be in the order of about 0.5ns. A nanosecond is a **billionth** of a second, hardly a span of time we experience in our day to day lives. Let's scale it up a bit and imagine that register ops took about a minute instead. In this hellish reality, a memory read would take 3 hours and 20 minutes. Enough time to watch the whole movie The Lord of The Rings: Fellowship of the Ring (extended edition of course) while you wait!

So, why go deferred at all if we'll be wasting so much time writing data to memory and then reading it again in a second shader program? We've already touched upon some of the benefits for multi-pass setups, but in a **single-pass** setup &mdash; shown in the pseudocode below &mdash; performance gains happen for different, but much more important, reasons. 

{% highlight c# %}
//Buffers:
Buffer display
Buffer GBuffer 

//Shaders:
Shader manyLightShader
Shader writeShadingAttributes

//Visibility & materials
for mesh in scene
    if mesh.depth < GBuffer.depth
       GBuffer = writeShadingAttributes(mesh)

//Shading & lighting - Single-pass
display = manyLightShader(GBuffer, scene.lights)
{% endhighlight %} 

In a single-pass setup we evaluate all of the lights in a scene using only a single large shader. This allows us to read the G-Buffer only once and handle all of the different types of lights using dynamic branching. This approach improves upon some of the shortcomings of multi-pass systems by bypassing the need to load shading attributes multiple times. Single-pass shading is also compatible with forward rendering, but can lead to its single shader program &mdash; sometimes referred to as "uber-shader" &mdash; becoming slow and unwieldy due to its many responsibilities.

And therein lies the biggest perk of deferred shading, it reduces the number of responsibilities of a shader through **specialization**. By dividing the work between multiple shader programs we can avoid massively branching "uber-shaders" and reduce register pressure &mdash; in other words, reduce the amount of variables a shader program needs to keep track of at a given time. 

But there are some other drawbacks to classical deferred methods that we haven't discussed yet. For starters, transparency is problematic because G-Buffers only store the information relevant to a single mesh per pixel. This can be overcome by performing a secondary forward pass for transparent meshes but requires extra work per frame. Another major flaw is that there is no easy way to make use of hardware based anti-aliasing(AA) techniques like Multi-Sampled Anti-Aliasing (MSAA). However, this is not as big of a deal as it used to be thanks to the progress made in the field of Temporal AA. Lastly, the shading attributes that are stored within the G-Buffer are decided by the shading model that will be used later on in the lighting pass. Varying shading models is costly since it requires either large G-Buffers that can accommodate the data for all models or the implementation of a more complex ID/masking setup. 

However, bandwidth bottleneck concerns and large GPU memory usage continues to overshadow all the issues above and remains as the major drawbacks to classical deferred systems. An avenue of optimization that we have yet to cover is efficient light culling. So far, for both forward and deferred system in both their multi-pass and single-pass variants, we have evaluated lighting for every light at every pixel on the display. This includes pixels that were never in reach of any lights and therefore needlessly issued G-Buffer reads that consumed some of our precious bandwidth. For the next two sections we'll be looking at ways of efficiently identifying the lights affecting a given pixel.

### **Tiled Shading & Forward+**
<p></p>
Now, we enter the realm of efficient light culling, instead of checking all lights for all meshes we perform the checks for all screen space tiles. This is essentially a form of collision detection since we can give lights a volume based on their limited range, from the inverse square law. And then check those against the frustum of the camera. Now as you can imagine one of the variables at play here is the size of the tiles, so how do we determine that? well it is a matter of two opposing forces, ideally you want them to be as small as possible, that is pixel size, in which case you would be evaluating only the lights that are relevant, but if you do this, threads in the GPU would be each  possibly doing different things, so you would have huge divergence in the threads and no work would be saved. You could also perform this check in the cpu using SIMD but I think there are pretty large benefits to keeping all these operations on the cpu since you are reducing the amount of api calls and have to sync less often.

So finding this perfect tile size will be a matter of dtermining the optimal group size and occupancy for your gpu and also a matter of seeing if you are gpu or cpu bound already. In other words, just keep profilinig. Now to visualize the process I've included some pseudocode and an animation that kind of illustrates the general principles of the process:

Another issue can occur when you have a significant amount of lights in a scene since in a multi-pass system this will incur as many vertex and fragment shader calls as there are lights per object, which can very quickly overwhelm any GPU.

So as I was saying, all efficient shading algorithms are trying to solve the problems of overdraw and dealing with large numbers of light with the objective of transforming the problem from a O(lights * meshes) to a O(meshes + lights). We'll begin by evaluating the main strategies that you can use to reduce overdraw issues.


Lastly, it is important to remark that the current GPU performance trends indicate that the rate of growth of [compute capabilities continually outpace memory bandwidth growth](https://newq.net/dl/pub/SA2014ManyLightIntro.pdf). Hence compute bound algorithm are of more interest to us than algorithms with a memory  bandwidth bottleneck. This is something worth keeping in mind come tiled and clustered shading, since they are targeted as algorithms that can reduce the high memory bandwidth costs found in pure deferred renderers.

$$ O(\text{mesh}* \text{lights}) $$ algorithm.

<iframe id="tiled1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/tiled.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>


{% highlight c# %}
//Buffers:
Buffer display
Buffer GBuffer 
Buffer tileArray

//Shaders:
Shader shade
Shader writeShadingAttributes
CompShader lightInTile

//Visibility & materials
for mesh in scene
    if mesh.depth < GBuffer.depth
       GBuffer = writeShadingAttributes(mesh)

//Light culling
for tile in tileArray
   for light in scene
      if lightInTile(tile, light)
          tile += light
      
//Shading
display = shade(GBuffer, tileArray)
{% endhighlight %} 


Notice how this third loop we have included essentially decouples the process of finding if a light affects a pixel from the actual process of shading it. At this point we are very close to achieving the dream of making this a O(lights + meshes) but there is still some large view dependency happening. This happens because tiles aren't actually just 2D cuts of the screen but actually they cover the full frustum from the near plane to the far plane. This means that tiles with large depth discontinuities, that is tiles with a large difference between the max and min depth they contain. A typical example given of a scene that creates view issues is a street with lined street lamps. The example below shows the issues of depth discontinuities.

<iframe id="depthDisc1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/depthDisc.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

The problem here is that the tiles aren't really 2D tiles but actually are these elongated thin frustums that extend from the near plaen to far plane.

Specifically the problem can be seen with the yellow light is that in the center tiles there is a depth difference between the ico sphere and the large sphere in the back which causes issues because the yellow light is further back but all the pixels in the front will also be checking for it, which kills perf. Something worth noting is that forward does not necessarily have to be done using deferred, forward methods are also possible but require a z-prepass. Which really is kind of a very very light deferred method so really who knows. So how can we fix the issues of large discontinuities? Well, the simplest way is to stop using a representation that is nto really adequate for 3D scenes, that is to better deal with depth issues we have to move from a 2d to 3D.

Also another important point is that tiled and clustered methods are not necessarily only used with deferred methods, they can also be applied to forward methods although normally you just use a z prepass. I already said this lol.

### **Clustered Shading**
<p></p>

Clustered shading is the representation of that idea, the world we are trying to represent is 3D yet we are tiling in 2D and not taking into account the fact that we already know the shape and size of the view frustum before rendering. Because we already know this shape and size we can very easily subdivide it according to that and build a list of clusters that we can reference instead. It was first introduced by [Ola Olsson et al. in Clustered Deferred and Forward Shading](http://www.cse.chalmers.se/~uffe/clustered_shading_preprint.pdf) and I highly recommend you read this paper since it is pretty approacheable overall. We'll be going into a lot more detail in the second part of this post so I'll just cover the main ideas briefly here and we'll go into more specific details later.

<iframe id="clus1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/clustered.html" style = "width:100%" frameborder="0" scrolling="no"></iframe>

{% highlight c# %}
//Buffers:
Buffer display
Buffer GBuffer
Buffer clusterArray

//Shaders:
Shader shade
Shader writeShadingAttributes
CompShader lightInCluster 

//Visibility & materials
for mesh in scene
    if mesh.depth < GBuffer.depth
       GBuffer = writeShadingAttributes(mesh)

//Light culling
for cluster in clusterArray
   for light in scene
      if lightIncluster(cluster, light)
          cluster += light

//Shading
display = shade(GBuffer, clusterArray)

{% endhighlight %} 

Notice how similar the pseudocode is when you compare it to tiled. This is actually pretty representative of what happened when I wrote code that went from tiled to clustered since essentially the concepts behind it are really similar and the shaders you are using to draw don't really care how the lights are stored, just that they can easily access it from a given light. We will go into more detail as to how to create thsese clusters in the following section and exactly how to subdivide the lights, but something worth mentioning that clustered methods have above tiled methods is that you do not really need to do a z-prepass anymore, although most likely you will do it anyway. The main difference between tiled and clustered really is this whole subdivision of the z frustum to more optimally group lights. There is obviously much tath could still be improved in this process, for example, instead of checking every cluster for every light we could predetermine if a cluster is active via a z-prepass and then only evaluate all of the lights for the active clusters. Or to go even further instead of evaluating all of the lights we could use some spatial acceleration structure like a BVH to perform the light culling step considerably faster!

I hope that now you have gained an intuition for these algorithms and what they are trying to do. This is not intended to be enough for you to build your own algorithms but enough for you to undertsand the more advanced papers and get a feel for what is happening. 

|                     | Simple forward | Simple Deferred |
|---------------------|----------------|-----------------|
| Geometry Passes     | 1              | 1               |
| Light Passes        | 0              | 1/light         |
| Transparency        | yes            | no              |
| MSAA                | yes            | no              |
| Bandwidth           | low            | high            |
| Register pressure   | high           | low             |
| Vary shading models | easy           | hard            |

<p></p>

|                     | Tiled/Clustered Deferred | Tile/Clustered Forward |
|---------------------|--------------------------|------------------------|
| Geometry Passes     | 1                        | 1-2                    |
| Light Passes        | 1                        | 1                      |
| Transparency        |no (yes if fwd)           | yes                    |
| MSAA                |no                        | yes                    |
| Bandwidth           |better                    | low                    |
| Register pressure   |low                       | yes                    |
| Vary shading models |hardish                   | easy                   |

<p></p>

The table above summarizes the pros and cons of each method as we have discussed, the finer details willl give you a much better overview of exactly what the pros and cons are to each method. But it is a good rule of thumb to look at each of these categories and think which are important for you and then look at which algorithm has a positive mark in that category. Although this has been a 10kft overview of each algorithm I think it is enough to understand the motivation between each of them so you can move on to am ore complex one like clustered shading with at least some background asa to where it is coming from.

#### Further reading:

* [Rendering an Image of a 3D Scene by ScratchAPixel](https://www.scratchapixel.com/lessons/3d-basic-rendering/rendering-3d-scene-overview/computer-discrete-raster)
* [A Short Introduction to Computer Graphics by Fredo Durand](http://people.csail.mit.edu/fredo/Depiction/1_Introduction/reviewGraphics.pdf)
* [Beyond porting by Cass Everitt and John McDonald](http://media.steampowered.com/apps/steamdevdays/slides/beyondporting.pdf)
* [Clustered Forward vs Deferred Shading by Matias](http://www.yosoygames.com.ar/wp/2016/11/clustered-forward-vs-deferred-shading/)
* [Forward vs Deferred vs Forward+ by Jeremiah van Oosten](https://www.3dgep.com/forward-plus/)
* [The real-time rendering continuum by Angelo Pesce](http://c0de517e.blogspot.com/2016/08/the-real-time-rendering-continuum.html)
* [Chapter 20: Efficient Shading in Real Time Rendering 4th Edition](http://www.realtimerendering.com/)
* [Latency numbers every programmer should know by HellerBarde](https://gist.github.com/hellerbarde/2843375)
* [Deferred Shading Optimizations By Nicolas Thibieroz](http://twvideo01.ubm-us.net/o1/vault/gdc2011/slides/Nicolas_Thibieroz_Programming_Deferred_Shading_Optimizations.pps)
* [Forward+: Bringing deferred lighting to the next level by Takahiro Harada et al.](https://takahiroharada.files.wordpress.com/2015/04/forward_plus.pdf#page=2&zoom=100,0,78)
* [Parallel Computer Architecture and Programming by Prof. Kayvon Fatahalian ](http://15418.courses.cs.cmu.edu/tsinghua2017/)
* [Clustered Deferred and Forward Shading by Ola Olsson et al.](http://www.cse.chalmers.se/~uffe/clustered_shading_preprint.pdf)

---
<p></p>
# ***Part 2:***
<p></p>
## Clustered Hybrid Shading
<p></p>
---
*...and **versatile** rendering ...*
*... and **quickly computes** a ...*
*...a **3D grid of blocks or "clusters"** and ...*
*...each **Active Volume**...*

The following section consists on a more in detailed review of Clustered shading beginning with a backgorund on when it was created and it's use case. It was first introduced by Ola Olsson et al in the 2012 paper titled "Clustered deferred and forward shading" it has already been used in multiple major releases since and is now the algorithm of choice in many games and engines, as I'll show in a section below. The algorithm can be summarized in the following steps:

* 0: Initialize build clustered data structures in GPU
* 1: Render scene to GBuffer or Z-prepass as normal
* 2: Discover which clusters are currently in view by evaluating each screen pixel and checking which cluster it belongs too
* 3: Reduce the list of clusters to just uniques
* 4: Perform light culling and assign lights to clusters
* 5: Shade samples using light list

Step 1 and 5 are out of the scope of this post since it is really dependent on whatever shading model you're using either deferred or forward or whateverr it is you do. Let's just say that our job here is to simply connect the pipes between the clusters and the fragment shader but not go beyond that. let's start by looking at step zero and work our way through the restf:

### **Building a Cluster Grid**
<p></p>

There are many possible ways of clustering data together but the easiest way and one of the most beneficial is to simply cluster together samples that are close together. The advantage in this is that objects that are close togther physically tend to be affected by the same lights, and therefore by picking samples that are close together we make sure that those samples sahre similar lights. So what we will do is subdivide our view space into clusters, first by performing a 2D subdivision in screen space and then subdividing even further in depth. How to exactly perform this depth subdivision is also up to the algorithm implementer but in the paper they briefly examine three possible ways to do it. 

<iframe id="zslice1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/zslices.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

The animation above showcases these three possible subdivision schemes, subdivision in NDC, regular view frustum subdivision and exponential subdivision, in order of appearance. The first one presents issues because although the segments are uniform in NDC, NDC itself is rather non-uniform in view space. This results in very thin clusters close to the camera and very large clusters close to the far plane. The opposite occurs in uniform subdivision, where near clusters are long and narrow and those far awaya re very wide and flat. The next attempt consists in making them exponentially distributed in such a way that we counteract the non-linearity of transforming NDC space to view space. The paper then goes ahead and introduces their equation for non linear view space which you can find below:

$$\begin{equation} k = \left\lfloor\dfrac{ \log\left(-Z_{vs} / \text{near}\right)}{\log\left(1 +  \frac{2 tan \theta}{S_y}\right)}\right\rfloor \end{equation}$$ 

In my implementation I ended up using a logarithmic distribution based on the one talked about in the DOOM siggraph presentation which you can see here:

$$\begin{equation} \text{Z} = \text{Near}_z \times \left(\frac{\text{Far}_z}{\text{Near}_z} \right)^{\text{slice} / \text{numSlices}} \end{equation}$$ 

We can solve find which specific slice a pixel is in from their given Z depth using 

$$\begin{equation} \text{slice} = \left\lfloor\ \log\left( Z\right)\frac{\text{numSlices}}{\log\left( \text{Far}_z / \text{Near}_z\right)} -  \frac{\text{numSlices} \times \log\left( \text{Near}_z \right)}{\log\left( \text{Far}_z / \text{Near}_z\right)}  \right\rfloor \end{equation}$$ 

Which I have also plotted in Desmos and you can play with in [ this link ](https://www.desmos.com/calculator/bf0g6n0hqp) or just take a look at in  the plot below.

<iframe id="zslice2" src="https://www.desmos.com/calculator/msnl9ob6tx?embed" style="width:100%" frameborder="0"></iframe>

Here you can check out what this depth slicing looks like in a scene like Sponza, taken directly from my engine. The colors represent the index representation in binary looping every 8 z depths. 

<div class="img">
	<img class="col gif" src="{{ site.baseurl }}/img/CG/clusters.png">
</div>

 
So how exactly do you calculate these z clusters? Well, you can find the specific implementation I used in the following [link](https://github.com/Angelo1211/HybridRenderingEngine/blob/master/assets/shaders/ComputeShaders/clusterShader.comp). It is written in a glsl compute shader, so it might seem like nonsense if you are not familiar with it but the general idea of the algorithm is to do the following: 

Also picking the tile size is kind of arbitrary, so just experiment to see what works best in your system although it probably will be a tile size that evenly divides the screen to avoid wasted threads.


{% highlight c# %}
//Each cluster has it's own thread ID in x, y and z and we dispatch one thread per cluster
for each cluster
    //Eye position is zero in view space
    const vec3 eyePos = vec3(0.0);

    //Per cluster variables
    uint tileSizePx = tileSizes[3];
    uint tileIndex  = gl_GlobalInvocationID;

    //Calculating the min and max point in screen space
    vec4 maxPoint_sS = vec4(vec2(gl_WorkGroupID.x + 1,
                                 gl_WorkGroupID.y + 1) * tileSizePx,
                                 -1.0, 1.0); // Top Right
    vec4 minPoint_sS = vec4(gl_WorkGroupID.xy * tileSizePx,
                            -1.0, 1.0); // Bottom left

    //Pass min and max to view space
    vec3 maxPoint_vS = screen2View(maxPoint_sS).xyz;
    vec3 minPoint_vS = screen2View(minPoint_sS).xyz;

    //Near and far values of the cluster in view space
    //We use equation (2) directly to obtain the tile vales
    float tileNear  = -zNear * pow(zFar/ zNear, gl_WorkGroupID.z/float(gl_NumWorkGroups.z));
    float tileFar   = -zNear * pow(zFar/ zNear, (gl_WorkGroupID.z + 1) /float(gl_NumWorkGroups.z));

    //Finding the 4 intersection points made from the maxPoint to the cluster near/far plane
    vec3 minPointNear = lineIntersectionToZPlane(eyePos, minPoint_vS, tileNear );
    vec3 minPointFar  = lineIntersectionToZPlane(eyePos, minPoint_vS, tileFar );
    vec3 maxPointNear = lineIntersectionToZPlane(eyePos, maxPoint_vS, tileNear );
    vec3 maxPointFar  = lineIntersectionToZPlane(eyePos, maxPoint_vS, tileFar );

    vec3 minPointAABB = min(min(minPointNear, minPointFar),min(maxPointNear, maxPointFar));
    vec3 maxPointAABB = max(max(minPointNear, minPointFar),max(maxPointNear, maxPointFar));

    //Saving the AABB at the tile linear index
    //Cluster is just a SSBO made of a struct of 2 vec4's
    cluster[tileIndex].minPoint  = vec4(minPointAABB , 0.0);
    cluster[tileIndex].maxPoint  = vec4(maxPointAABB , 0.0);
                                                                                                                
{% endhighlight %}

There are two functions in this process worth highlighting: the line intersection to plane and the screen2View function. The screen 2 view function converts a given point on screen space to view space by taking the reverse transformation steps of the graphics pipeline. We begin from Screen space, transsform to NDC then to clipspace and lastly to view space by using the inverse projection matrix and dividing by w. I found this function in [Matt Pettineo's Blog THE DANGER ZONE](https://mynameismjp.wordpress.com/2009/03/10/reconstructing-position-from-depth/), thanks Matt!

{% highlight c# %}
vec4 screen2View(vec4 screen){
    //Convert to NDC
    vec2 texCoord = screen.xy / screenDimensions.xy;

    //Convert to clipSpace
    vec4 clip = vec4(vec2(texCoord.x, 1.0 - texCoord.y)* 2.0 - 1.0, screen.z, screen.w);

    //View space transform
    vec4 view = inverseProjection * clip;

    //Perspective projection
    view = view / view.w;
    
    return view;
}
{% endhighlight %}

To obtain the points on the cluster we use a line intersection to plane function, since we already know the plane normal from the fact we are working in viewspace, we also know the the origin of the camera in viespace is zero. The algorithm is kind of self explanatory here:

{% highlight c# %}
  //Creates a line from the eye to the screenpoint, then finds its intersection
  //With a z oriented plane located at the given distance to the origin
  vec3 lineIntersectionToZPlane(vec3 A, vec3 B, float zDistance){
      //all clusters planes are aligned in the same z direction
      vec3 normal = vec3(0.0, 0.0, 1.0);
      //getting the line from the eye to the tile
      vec3 ab =  B - A;
      //Computing the intersection length for the line and the plane
      float t = (zDistance - dot(normal, A)) / dot(normal, ab);
      //Computing the actual xyz position of the point along the line
      vec3 result = A + t * ab;
      return result;
  }

{% endhighlight %}

Why do we use AABB's instead of tight frustums? Well, AABBS built from the max and min points will be ever so slightly larger than the actual clusters, this ensures that there are no gaps in between the clusters. Using AABB's also allows for storage of the cluster using only 2 vec3's, one for each corner of the AABB. Which reduces bandwidth significantly. Once we have AABB's for each cluster we are ready to use it in our per-frame light culling and active cluster determination schemes. These AABB's will be valid for as long as the frustum stays within this shape. Any changes in FOV or near and far plane settings will require a full frustum reconstruction, but I think it might not be a huge deal since RenderDoc seemed to tell me this function was not too expensive in terms of performance costs. It needs to actually be tested to see, maybe getting more clusters would screw up things not sure yet



### **Determining Active Clusters**
<p></p>

This part is rather optional, since we can just check all clusters for all lights for now. However, if you do go for this you will have to do a ZPre-pass, so this is only really worth it if you were doing this anyway for some reason. Now, I have not fully implemented this yet, it is planned for a future upgrade of my engine but a naive implementation of it in could be done like so:

{% highlight c# %}
//We will evaluate the whole screen in one compute shader 
//so each thread is equivalent to a pixel
void markActiveClusters(){
    vec2 screenCord = pixelID.xy / screenDimensions.xy;
    float z = texture(screenCord)
    uint index = getClusterIndex(pixelID.xy, z);
    clusterActive[index] = true;
}
{% endhighlight %}

<p></p>

{% highlight c# %}
uint getClusterIndex(vec3 pixelCoord){
    // Using equation (3) from the cluster determination section
    clusterZVal       = getDepthSlice(pixelCoord.z);
    uvec3 clusters    = uvec3( uvec2( pixelCoord.xy / screenDimensions.xy ), zTile);
    uint clusterIndex = clusters.x +
                        screenDimensions.x * clusters.y +
                        (screenDimensions.x * screenDimensions.y) * clusters.z;
    return clusterIndex;
}
{% endhighlight %}
With this compute shader we would mark all the active clusters in a grid simply by writing to an array of bools as large as the cluster grid and then we would jsut set the flag as true if it is active at any point. To increase efficiency both Van Oosten and Olsson compact this list into a set of unique clusters to check from. This helps because it reduces the amount of clusters that need to have lights culled against them, instead of having to cull every light to every cluster, which would make the problem O(lights * clusters), remember that this was originally O(meshes * lights) so by using a coarser world grid of clusters we significantly improve performance if clusters < meshes.


{% highlight c# %}
//We will every cluster in one compute shader
void buildCompactClusterList(){
    uint tileIndex  = gl_GlobalInvocationID;
    if(clusterActive[index]){
       uint offset = atomicAdd(globalActiveClusterCount, 1);
       uniqueActiveClusters[offset] = tileIndex;
    }
}
{% endhighlight %}

The global active cluster count variable keeps track of how many active clusters there are currently in the scene, and the unique active clusters array is a linear array with a size equal to the amount of clusters and will be rest to zero before calling this function. This way we now have an index of how many active clusters there are and we can launch computes in an indirect manner using this size. Once this is done we now have succesfully completed the active cluster and unique list of cluster determination step and we can move to light culling.

### **Light Culling Methods**
<p></p>

So because I think this visualization looks pretty cool I'd like to include it again so we can discuss it in some more detail.

<iframe id="clus2" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/clustered.html" style = "width:100%" frameborder="0" scrolling="no"></iframe>

So the light culling compute shader is much larger and would not fit properly in this post and make it much larger than it already is, so [here's a link](https://github.com/Angelo1211/HybridRenderingEngine/blob/master/assets/shaders/ComputeShaders/clusterCullLightShader.comp) At this point, remember, we have a list of active clusters, their AABB's and a list of lights in the scene. With this and a know radius of effect of a light we can simply perform collision detection in parallel for each light against each active cluster. The data structure used is somewhat convoluted, so I won't cover it in much detail. The main idea is something like this:

* DS1: **Cluster Grid Light list** (size of numClusters):
    * Offset to beginning of light Index DS
    * Number of lights in cluster
* DS2: **Light Index List** (size of numClusters * worst case scenario light per cluster (100)**
    * Index to scene light list
* DS3: **Scene light list** (size of number of lights in scene)
    * Struct of point lights
       * Position
       * Color
       * Enabled
       * Range 

These data structures are necessary to perform the checks in parallel effectively. What essentially ends up happeneing is that lights get grouped spatially and their indexes get added to the light index list.

In pseudocode the process goes something like this, and warning, it's kind of hard to follow because I was looking for max performance by interchangeably using the same variables to mean different things in different contexts, for example although batch refers to a batch of lights it can also refer to a batch of clusters. I'm sorry but this is the best I could come up with that was fast. 

{% highlight c# %}
// This is the gist of what is happening in the compute shader, but simplified a bit
void main(){
    for cluster in scene{
        for light in scene {
            localLightArray[100];
            localLightCount = 0;
           

            // Add to a cluster local list of lights
            if light.enabled == 1{
                if light in cluster{
                    localLightArray[lightCount]  = light;
                    localLightCount++;
                }
            }
        }
        
        //Make sure all clusters have finished evaluating 
        //all lights at the same time
        barrier();
        
        //The atomic addition to the global light count will return
        //the previous value before adding, we can use this as an 
        //offset into the global light index list
        offset = atomicAdd(globalLightCount, localLightCount); 
        
        //For each light in the local list add it sequencially to the
        //global light index list starting from the offset 
        for index in range(0, localLightCount){
            globalLightIndexList[offset + index] = localLight[index];
        }
        
        //Fillin in the cluster grid light list with the offset and count
        //to traverse the light index list properly
        lightGrid[cluster].offset = offset;
        lightGrid[clustre].count  = localLightCount
    }
}
{% endhighlight %}

Once this shader is run the data structures mentioned above will be filled with the values necessary for a shader to obtain teh lights that are affecting a given cluster. It can be easily accessed from the fragment shader if we include a function that can directly obtain the given cluster from a screen value and a depth, like the one we found in the cluster determination section!
### **Optimization Techniques**
<p></p>

Once you have a working implementation there is still much that can be done to improve performance, van oosten [link](https://www.3dgep.com/volume-tiled-forward-shading/) shows specifically how spatial optimization structures like BVH can significantly increase the light culling step to the point where up to a million lights can be used in real time performance.

<iframe id="video1" style="width:100%" src="https://www.youtube.com/embed/nyItqF3sM84" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


Olsson [link](http://www.cse.chalmers.se/~uffe/ClusteredWithShadowsSiggraphTalk2014.pdf) presents various methods that leverage the benefits of clustered shading, one of them focuses on making these many lights be shadow casting using virtual shadow mapping and another presents and alternate method of clustering using the normal at the surface instead of the world space position. 

There are actual applications for renderers that use that many lights. Specifically in the field of global illumination, where you can fake indirect lighting by using many small lights instead. Another improvement we can make with clustered rendering is to do what DOOM 2016 did and include many different things in the clusters, not just lights but also decals and environment probes. This opens up the possibility of using this very coarse voxel grid to perform any kind of world space voxelization algorithms. 

Also something that is metnioned in the papers is that the strength of clustered shading is that it allows to turn splatting techniques into gathering techniques. There is also a lot of talk these days about gpu level optimizations you can do at the wave level which I have not researched in depth yet but would like to soon at some time. I might go into this at some point in my project or not, I am not sure yet

<p></p>
### **Successful Implementations**
<p></p>

So I would be remiss not to end this post by sharing some great presentations from other more experienced devs where they discuss their own versions of Clustered shading. As you can imagine, no two are alike, both in the nitty-gritty details of their implementations and in the large scale differences found in the genres of the games they make. It speaks volumes for the flexibility of this algorithm and hints towards a future where hybrid forward/deferred engines are the norm for AAA products that demand the highest visual fidelity. 

Also, hey, I've technically implemented a Clustered Renderer too! If you've forgotten about it after reading this whole post I don't blame you, but [here's a link](https://github.com/Angelo1211/HybridRenderingEngine) for you so you don't have to scroll all the way to the top. It's a simplified implementation with much of the code commented so you &mdash; and I in about a year when I forget &mdash; can follow along easily. Okay okay, enough self promotion, here are the goods:

* [Clustered Shading in the Wild by Ola Olsson](http://efficientshading.com/blog/)
* [(Doom 2016)The devil is in the Details by Tiago Sousa and Jean Geffrey](https://www.slideshare.net/TiagoAlexSousa/siggraph2016-the-devil-is-in-the-details-idtech-666?next_slideshow=1)
* [Practical Clustered Shading by Emil Persson](http://www.humus.name/Articles/PracticalClusteredShading.pdf)
* [Clustered Forward Rendering and Anti-aliasing in 'Detroit: Become Human' by Ronan Marchalot](https://twvideo01.ubm-us.net/o1/vault/gdc2018/presentations/Marchalot_Ronan_ClusteredRenderingAnd.pdf)

#### Further reading:
* [Chapter 20: Efficient Shading in Real Time Rendering 4th Edition](http://www.realtimerendering.com/)
* [Volume Tiled Forward Shading by Jeremiah Van Oosten](https://www.3dgep.com/volume-tiled-forward-shading/)
* [Practical Clustered Shading By Emil Persson](http://newq.net/dl/pub/SA2014Practical.pdf)
* [Tiled and Clustered Forward Shading Supporting Transparency and MSAA Olsson et al.](https://www.researchgate.net/publication/254463291_Tiled_and_Clustered_Forward_Shading_Supporting_Transparency_and_MSAA)
* [Adrian Courreges' Doom (2016) - Graphics Study](http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/)
* [Efficient Virtual Shadow Maps for Many Lights by Ola Olsson et al.](http://www.cse.chalmers.se/~uffe/ClusteredWithShadows.pdf)

---
*Psst, hey you! If you've gotten this far I'd like to thank you for taking the time to read this post. I hope you enjoyed reading it as much as I did writing it. Just letting you know that I'm currently looking for a Graphics related position. If you're interested or know of any opportunities, I'd appreciate it if you'd let me know. Feel free to check out my "about" page for a choice selection of social media where you can reach me!*

*Any questions? Dead links? Complaints to management? My email is angelo12@vt.edu but you can also find me on twitter at [@aortizelguero](https://twitter.com/aortizelguero).*

