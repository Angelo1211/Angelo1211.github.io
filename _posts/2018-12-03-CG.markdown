---
layout: post
title:  A Brief Primer On Efficient Rendering Algorithms & Clustered Shading.
date:   2018-12-02 23:13:40
description: Introducing the background and concepts needed to understand the subject and taxonomy of efficient shading algorithms.
---

### **Table of Contents**
---

* [**Part 1: Efficient Rendering Algorithm**](#part-1)
    * [Forward Shading](#forward-shading)
    * [Deferred Shading](#deferred-shading)
    * [Tiled Shading & Forward+](#tiled-shading--forward)
    * [Clustered Shading](#clustered-shading)

* [**Part 2: Clustered Hybrid Shading in Depth**](#part-2)
    * [Building a Cluster Grid](#building-a-cluster-grid)
    * [Determining Active Clusters](#determining-active-clusters)
    * [Light Culling Methods](#light-culling-methods)
    * [Optimization Techniques](#optimization-techniques)
    * [Successful Implementations](#successful-implementations)

---
<p></p>
For the past three months I’ve been fully immersed in the world of computer graphics by working full-time on a 3D renderer built with OpenGL. I’m pretty happy with how it’s coming along so far and although there’s still a handful of features I’d like to include, I finally feel comfortable enough with the final result to share with the world in its current semi-presentable state.I've uploaded it to GitHub and you can [check it out here](https://github.com/Angelo1211/HybridRenderingEngine).
It is currently only available on Windows platforms, but who knows, I might get it to work on Linux at some point soon, fingers crossed.

It comes with all the familiar bells and whistles you've probably come to expect from the multitude of hobby renderers out there: shadow mapping, MSAA, normal mapping - you name it, if it has an online tutorial it's probably in there. However, among the sea of entry level features there's one in particular that I feel is not, so to speak, "stock", and of which I'm proud enough to have convinced myself that it is worthy of its own post. I'm of course talking about my own personal implementation of **Clustered shading**. An algorithm which I first encountered in [Adrian Courreges' Doom (2016) - Graphics Study](http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/), which if you haven't already read, I highly recommend you do before continuing, since it's simply a fascinating read, and his explanation of clustered shading is a great entry point.

I read it not too long ago, when I was halfway done building my renderer &mdash; although at the time I would have told you I was 80% done &mdash; and at a point where I felt pretty great about myself. I had finally left the dark, dark, hopeless pit that is getting a "correct" shadow mapping implementation working, and it certainly felt like all was going to be smooth sailing from now on. Maybe that's why my first thoughts when I read the section on Clustered shading were something along the lines of: "hmm, this seems pretty neat! I wonder how hard it REALLY could be to implement? I mean, it's kind of like culling but with some extra steps and all done on the GPU. I know how to write regular shaders, compute shaders can't be so different! Shouldn't take too long."

Three weeks, [12 articles](https://github.com/Angelo1211/HybridRenderingEngine/wiki/References#11-shading-algorithm-overviews), 3 papers[[1]](http://www.cse.chalmers.se/~uffe/clustered_shading_preprint.pdf)[[2]](https://takahiroharada.files.wordpress.com/2015/04/forward_plus.pdf)[[3]](http://www.cse.chalmers.se/~uffe/ClusteredWithShadows.pdf) and [one](http://15418.courses.cs.cmu.edu/tsinghua2017/) and a [half](https://www.youtube.com/watch?v=F620ommtjqk&list=PLGvfHSgImk4aweyWlhBXNF6XISY3um82_) online courses later I can confirm, yep, it's still not as bad as shadow mapping.

But, in an effort to save you some of the time out of those three weeks I wanted to write an intro to Clustered shading that would explain the algorithm and some of its details in great depth. But in the process of writing it I constantly made references to some other previous algorithms to explain the benefits of Clustered. Since those references started growing to become a significant part of the article I've split this into two parts. Part one focuses only on the rendering algorithms which clustered shading is built upon and explain the reasoning behind them, what problems they're trying to solve and how they go about solving them. We'll begin with a quick summary of Clustered shading that kind of condenses the "soul" so to speak, of the algorithm and then proceed to break down each part in the proceeding sections.


### **TL;DR**
---
<p></p>
> *Cluster shading is an efficient and versatile rendering algorithm capable of unified lighting for both forward and deferred shading systems. It divides the view frustum into a 3D grid of blocks or "clusters" and quickly computes a list of lights intersecting each active volume.*

---
*You can fit this in a tweet!*

<br/>

# ***Part 1:***
<p></p>
## Efficient Rendering Algorithms
<p></p>
---
*...versatile **rendering algorithm** capable...*
*...an **efficient** and...*
*...a **list of lights** intersecting...*
*...both **forward and deferred shading systems**...*
*...the **view frustum** into a 3D grid ...*

So what exactly makes a rendering algorithm an **efficient rendering algorithm**? I did a bunch of research but the conclusion I finally got to was that efficient rendering algorithms are basically any algorithm that you use to render that isn't the most basic and naive forward renderer. So this obviously isn't a terribly descriptive description so we have to go more in detail, to do this we'll begin by evaluating the most basic possible algorithm and build up from it. 


It goes without saying &mdash;yet I will anyway&mdash; that some of these algorithms are only really worth implementing in projects where pixel shader evaluations are becoming a bottle neck and there are concerns about performance and maintaning a stable frame rate. There ain't no such thing as a free lunch, and there are side-effects to **all** of these algorithms that will add an overhead to the rendering process. It really might not be worth the engineering time you'll have to spend on them until your fragment shaders have become pretty large an unwieldy, so make sure to consult your ~~[primary care phycisian](https://renderdoc.org/)~~ profiler tools to assess the location of your current bottleneck.  

### **Forward Shading**
<script>
    $( document ).ready( function(){
        var cw = $('#forward1').width() * 0.80 ;
        $('#forward1').css({'height':cw  +'px'});
        $('#overdraw1').css({'height':cw  +'px'});
        $('#zpre1').css({'height':cw  +'px'});
        $('#def1').css({'height':cw  +'px'});
        $('#tiled1').css({'height':cw  +'px'});
        $('#depthDisc1').css({'height':cw * 2.1  +'px'});
        $('#clus1').css({'height':(cw * 2.1) +'px'});
        $('#zslice1').css({'height':cw  +'px'});
        $('#zslice2').css({'height':cw  +'px'});
    });
</script>
 
<iframe id="forward1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/forward.html" style = "width: 100%;" frameborder="0" scrolling="no"  ></iframe>

{% highlight c# %}
for mesh in scene
    if mesh.isVisible()
        for light in scene
           mesh.shade(light)
{% endhighlight %} 

Let's begin the  overview of efficient algorithms by talking about one that isn't. The animation and the pseudocode above illustrate at a pretty basic level the behaviour of a naïve **multi-pass** classical forward renderer. Let's break down this definition: I use the word "classical" here because this very roughly translates to the typical algorithm you would find in engines of the late 90's and early 2000's. Why it is called "Forward" I am not entirely sure, &mdash; please let me know if there is actually a non-obvious reason for the name &mdash; but if I were to guess it's beacause the mesh data marches *forward* along the GPU pipeline in a somewhat linear fashion. From the vertex shader to the rasterizer to the vertex shader to the buffer where our frame is being stored before being sent to the monitor. Multi-pass refers to the fact that we don't shade the object in one go, we iteratively shade it light by light using multiple simple fragment shaders for each light. As you can imagine, this simple approach still leaves a lot of room for improvement, but I wager that it would probably fair fine for scenes with few meshes, simple materials and just a couple of lights. 

The main issues with this algorithm become apparent when you increase the quantity or complexity of any of these variables. These are the core variables that each rendering algorithm are struggling with and trying to solve in different ways. On the one hand, trying to render complex scenes with classical forward shading will be problematic because of **overdraw**, which is essentially casued by meshes overwriting the values of previously drawn items because they appear closer to the viewer. Another issue can occur when you have a significant amount of lights in a scene since in a multi-pass system this will incur as many vertex and fragment shader calls as there are lights per object, which can very quickly overwhelm any GPU. Essentially, what we have here is a $$ O(\text{mesh}* \text{lights}) $$ algorithm.

<iframe id="overdraw1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/overdraw.html" style = "width:100%" frameborder="0" scrolling="no"  ></iframe>

You can see overdraw in action in the animation above. You spend all this time working on rendering a mesh and then, another mesh inconveniently comes along and places itself in front of your finished mesh and now you will have wasted a ton of fragment shader calls. The worst case scenario would occur if for some reason you are using the painter's algorithm to solve visibility and you've listed your meshes in a back to front order.

Overdraw is a specially egregious problem in Forward rendering since you have to go through the full vertex-fragment shader pipeline each draw call and you might end up issuing thousands of pixels worth of fragment shader calls that end up being thrown away when an object closer to the viewer gets drawn right on top of them. So how do we fix this issue? Well, the most obvious solution would be to sort the meshes front to back before rendering so that the z-buffer takes care of the problem. However, depending on your setup this might very likely be not worth the cost, since in the process of sorting you will have to go through the vertex transformation pipeline to get the Zdepth of each mesh anyway.

So as I was saying, all efficient shading algorithms are trying to solve the problems of overdraw and dealing with large numbers of light with the objective of transforming the problem from a O(lights * meshes) to a O(meshes + lights). We'll begin by evaluating the main strategies that you can use to reduce overdraw issues.


### **Deferred Shading**
<p></p>

So, the key insight to have here is that **there is no reason why you can't perform the visibility and shading steps separately**. The ideal case scenario is obviously one where you already know exactly what objects contribute to the final image and you only invoke the pixel shaders to shade whatever is visible of them. Sadly, there is no simple function that can determine ahead of time what objects will be displayed. &mdash;[Unless, of course, you are a modern age wizard](https://www.shadertoy.com/view/ldd3DX) &mdash; What we can do instead is run through every object in the scene as we did earlier, except this time, not shading them. Instead we'll keep track of the last object that affected a pixel in some way, normally by either recording their depth or their position at that given pixel in a texture. We can summarize this in pseudocode like so:
 


<div>
<iframe id="zpre1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/zprepass.html" style = "width:100%" frameborder="0" scrolling="no"  ></iframe>
</div>

{% highlight c# %}
texture depthTexture
for object in scene
    if object.isVisible() and (object.depth < depthTexture.depth)
       depthTexture.writeDepth(object)

for object in scene
    if object.depth == depthTexture.depth
        for light in scene
            object.shade(light)
{% endhighlight %}

This is usually known as a **Z-prepass**,  and it essentially *decouples* the process of determining the visibility of an object from the act of shading it. Notice that you haven't actually eliminated overdraw, you've just smartly allowed for it to happen at a point where it doesn't affect performance as much. Because in the process of filling in your depth texture you'll still end up writing over it multiple times. However, this is *generally* not as much of an issue since the cost of accessing rewriting the texture memory is much cheaper than performing a full lighting pass. Pretty neat right?

**Deferred rendering**, extends the main concept of the **Z-prepass** even further by not only writing the depth or position of a given object to textures, but also recording all of the attributes of an object that are used in shading. That includes various things such as the surface normal, the objects' albedo/color or it's specular/shininess. There really is no limit to what you can write in textures beyond the texture memory of the GPU itself. The collection of these textures is commonly referred to as the **G-Buffer** and we'll use it for the lighting pass directly instead of referring to the source objects. Once again I have to plug [Adrian Courreges's Blog](http://www.adriancourreges.com/blog/2017/12/15/mgs-v-graphics-study/) where he goes into more detail about Deferred Rendering in his post about MGS V. Let's update our pseudocode algorithm to resemble a simple deferred implementation:


<iframe id="def1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/deferred.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

{% highlight c# %}
Gbuffer buffer
for object in scene
    if object.isVisible() and (object.depth < buffer.depth)
       buffer.writeShadingAttributes(object)

for light in scene
    buffer.shade(light)
{% endhighlight %}

Yet deferred rendering is essentially making a trade-off between bandwidth and computation and something to keep in mind is that memory access is not cheap. In fact, if you were to go ahead and implement Deferred rendering as I've explained it so far, the performance gains would be pretty unremarkable, or in the case of large G-Buffers and many lights, performance would probably be worse. Even though you might have vanquished overdraw you have added a new cost to the shading & light accumulation step: reading the G-Buffer. Reading or writing memory in your tightest and most inner loop is always bad news, and even though GPU's have ways to hide the costs of memory reads, given enough lights, those latency hiding techniques will quickly become useless. If you're interested in understanding why [Prof. Fatahalian's course on Paralallel Computer architectures](http://15418.courses.cs.cmu.edu/tsinghua2017/lecture/basicarch) covers the modern Multi-core processor architecture, both for CPU's and GPU's, and explains memory latency and memory bandwidth in the best way I've found yet. 


> #### Small Detour: How slow are texture reads anyway?
> ---
> Texture reads are essentially memory reads and **reading memory is always slow**. How slow you ask? Well, there is no exact answer since it varies from architecture to architecture, but it is generally considered to be about ~200x slower than a register operation, which for the sake of argument let's assume takes ~0.5ns. A nanosecond is a **billionth** of a second, hardly a span of time we experience in our day to day lives. Let's scale it up a bit and imagine that register ops took about a minute instead. Say you're planning an evening with friends and invite them over to watch your favorite movie, The Lord of the Rings: The Fellowship of the ring! (extended edition of course, clocking at about 3h20m). As soon as the movie starts you ask one of your friends to grab you a drink from the fridge, by the time he gets back the credits start rolling. 

So we've tackled the problem of overdraw and found a simple way to determine if a surface will contribute to the final image. Next we're going to look into the methods that deal with lights and how to avoid having to evaluate every light in the scene at every pixel. In an ideal world there would be a way to cheaply identify all the lights that affect a given pixel and only evaluate those at a time, and we would make use of the fact that neighbouring pixels tend to be affected by the same lights. That's kind of what we're going for with all the stuff we're going to include next. 

### **Tiled Shading & Forward+**
<p></p>
Now, we enter the realm of efficient light culling, instead of checking all lights for all meshes we perform the checks for all screen space tiles. This is essentially a form of collision detection since we can give lights a volume based on their limited range, from the inverse square law. And then check those against the frustum of the camera. Now as you can imagine one of the variables at play here is the size of the tiles, so how do we determine that? well it is a matter of two opposing forces, ideally you want them to be as small as possible, that is pixel size, in which case you would be evaluating only the lights that are relevant, but if you do this, threads in the GPU would be each  possibly doing different things, so you would have huge divergence in the threads and no work would be saved. You could also perform this check in the cpu using SIMD but I think there are pretty large benefits to keeping all these operations on the cpu since you are reducing the amount of api calls and have to sync less often.

So finding this perfect tile size will be a matter of dtermining the optimal group size and occupancy for your gpu and also a matter of seeing if you are gpu or cpu bound already. In other words, just keep profilinig. Now to visualize the process I've included some pseudocode and an animation that kind of illustrates the general principles of the process:



<iframe id="tiled1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/tiled.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>


{% highlight c# %}
Gbuffer buffer
for mesh in scene
    if mesh.isVisible() and (mesh.depth < buffer.depth)
       buffer.writeShadingAttributes(mesh)

for tile in screen
   for light in scene
      if tile.interesects(light)
          tile.addLight(light)

for tile in screen
    for light in tile
        buffer.shade(light)
{% endhighlight %} 


Notice how this third loop we have included essentially decouples the process of finding if a light affects a pixel from the actual process of shading it. At this point we are very close to achieving the dream of making this a O(lights + meshes) but there is still some large view dependency happening. This happens because tiles aren't actually just 2D cuts of the screen but actually they cover the full frustum from the near plane to the far plane. This means that tiles with large depth discontinuities, that is tiles with a large difference between the max and min depth they contain. A typical example given of a scene that creates view issues is a street with lined street lamps. The example below shows the issues of depth discontinuities.

<iframe id="depthDisc1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/depthDisc.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

The problem here is that the tiles aren't really 2D tiles but actually are these elongated thin frustums that extend from the near plaen to far plane.

Specifically the problem can be seen with the yellow light is that in the center tiles there is a depth difference between the ico sphere and the large sphere in the back which causes issues because the yellow light is further back but all the pixels in the front will also be checking for it, which kills perf. Something worth noting is that forward does not necessarily have to be done using deferred, forward methods are also possible but require a z-prepass. Which really is kind of a very very light deferred method so really who knows. So how can we fix the issues of large discontinuities? Well, the simplest way is to stop using a representation that is nto really adequate for 3D scenes, that is to better deal with depth issues we have to move from a 2d to 3D.

Also another important point is that tiled and clustered methods are not necessarily only used with deferred methods, they can also be applied to forward methods although normally you just use a z prepass. I already said this lol.

### **Clustered Shading**
<p></p>

Clustered shading is the representation of that idea, the world we are trying to represent is 3D yet we are tiling in 2D and not taking into account the fact that we already know the shape and size of the view frustum before rendering. Because we already know this shape and size we can very easily subdivide it according to that and build a list of clusters that we can reference instead. It was first introduced by [Ola Olsson et al. in Clustered Deferred and Forward Shading](http://www.cse.chalmers.se/~uffe/clustered_shading_preprint.pdf) and I highly recommend you read this paper since it is pretty approacheable overall. We'll be going into a lot more detail in the second part of this post so I'll just cover the main ideas briefly here and we'll go into more specific details later.

<iframe id="clus1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/clustered.html" style = "width:100%" frameborder="0" scrolling="no"></iframe>

{% highlight c# %}
Gbuffer buffer
for mesh in scene
    if mesh.isVisible() and (mesh.depth < buffer.depth)
       buffer.writeShadingAttributes(mesh)

for cluster in screen
   for cluster in scene
      if cluster.interesects(light)
          cluster.addLight(light)

for cluster in screen
    for light in cluster
        buffer.shade(light)
{% endhighlight %} 

Notice how similar the pseudocode is when you compare it to tiled. This is actually pretty representative of what happened when I wrote code that went from tiled to clustered since essentially the concepts behind it are really similar and the shaders you are using to draw don't really care how the lights are stored, just that they can easily access it from a given light. We will go into more detail as to how to create thsese clusters in the following section and exactly how to subdivide the lights, but something worth mentioning that clustered methods have above tiled methods is that you do not really need to do a z-prepass anymore, although most likely you will do it anyway. The main difference between tiled and clustered really is this whole subdivision of the z frustum to more optimally group lights. There is obviously much tath could still be improved in this process, for example, instead of checking every cluster for every light we could predetermine if a cluster is active via a z-prepass and then only evaluate all of the lights for the active clusters. Or to go even further instead of evaluating all of the lights we could use some spatial acceleration structure like a BVH to perform the light culling step considerably faster!

I hope that now you have gained an intuition for these algorithms and what they are trying to do. This is not intended to be enough for you to build your own algorithms but enough for you to undertsand the more advanced papers and get a feel for what is happening. 

|                     | Simple forward | Simple Deferred |
|---------------------|----------------|-----------------|
| Geometry Passes     | 1              | 1               |
| Light Passes        | 0              | 1/light         |
| Transparency        | yes            | no              |
| MSAA                | yes            | no              |
| Bandwidth           | low            | high            |
| Register pressure   | high           | low             |
| Vary shading models | easy           | hard            |

<p></p>

|                     | Tiled/Clustered Deferred | Tile/Clustered Forward |
|---------------------|--------------------------|------------------------|
| Geometry Passes     | 1                        | 1-2                    |
| Light Passes        | 1                        | 1                      |
| Transparency        |no (yes if fwd)           | yes                    |
| MSAA                |no                        | yes                    |
| Bandwidth           |better                    | low                    |
| Register pressure   |low                       | yes                    |
| Vary shading models |hardish                   | easy                   |

<p></p>

The table above summarizes the pros and cons of each method as we have discussed, the finer details willl give you a much better overview of exactly what the pros and cons are to each method. But it is a good rule of thumb to look at each of these categories and think which are important for you and then look at which algorithm has a positive mark in that category. Although this has been a 10kft overview of each algorithm I think it is enough to understand the motivation between each of them so you can move on to am ore complex one like clustered shading with at least some background asa to where it is coming from.

#### Further reading:

* [Rendering an Image of a 3D Scene by ScratchAPixel](https://www.scratchapixel.com/lessons/3d-basic-rendering/rendering-3d-scene-overview/computer-discrete-raster)
* [A Short Introduction to Computer Graphics by Fredo Durand](http://people.csail.mit.edu/fredo/Depiction/1_Introduction/reviewGraphics.pdf)
* [Clustered Forward vs Deferred Shading by Matias](http://www.yosoygames.com.ar/wp/2016/11/clustered-forward-vs-deferred-shading/)
* [Forward vs Deferred vs Forward+ by Jeremiah van Oosten](https://www.3dgep.com/forward-plus/)
* [The real-time rendering continuum by Angelo Pesce](http://c0de517e.blogspot.com/2016/08/the-real-time-rendering-continuum.html)
* [Chapter 20: Efficient Shading in Real Time Rendering 4th Edition](http://www.realtimerendering.com/)
* [Hiding Stalls with multi-threading by Prof. Kayvon Fatahalian](http://15418.courses.cs.cmu.edu/tsinghua2017/lecture/basicarch/slide_052)
* [Latency numbers every programmer should know by HellerBarde](https://gist.github.com/hellerbarde/2843375)
* [Deferred Shading Optimizations By Nicolas Thibieroz](http://twvideo01.ubm-us.net/o1/vault/gdc2011/slides/Nicolas_Thibieroz_Programming_Deferred_Shading_Optimizations.pps)
* [Forward+: Bringing deferred lighting to the next level by Takahiro Harada et al.](https://takahiroharada.files.wordpress.com/2015/04/forward_plus.pdf#page=2&zoom=100,0,78)


---
<p></p>
# ***Part 2:***
<p></p>
## Clustered Hybrid Shading
<p></p>
---
*...and **versatile** rendering ...*
*... and **quickly computes** a ...*
*...a **3D grid of blocks or "clusters"** and ...*
*...each **Active Volume**...*
*... capable of **unified lighting** for ...*

The following section consists on a more in detailed review of Clustered shading beginning with a backgorund on when it was created and it's use case. It was first introduced by Ola Olsson et al in the 2012 paper titled "Clustered deferred and forward shading" it has already been used in multiple major releases since and is now the algorithm of choice in many games and engines, as I'll show in a section below. The algorithm can be summarized in the following steps:

* 0: Initialize build clustered data structures in GPU
* 1: Render scene to GBuffer or Z-prepass as normal
* 2: Discover which clusters are currently in view by evaluating each screen pixel and checking which cluster it belongs too
* 3: Reduce the list of clusters to just uniques
* 4: Perform light culling and assign lights to clusters
* 5: Shade samples using light list

Step 1 and 5 are out of the scope of this post since it is really dependent on whatever shading model you're using either deferred or forward or whateverr it is you do. Let's just say that our job here is to simply connect the pipes between the clusters and the fragment shader but not go beyond that. let's start by looking at step zero and work our way through the restf:

### **Building a Cluster Grid**
<p></p>

There are many possible ways of clustering data together but the easiest way and one of the most beneficial is to simply cluster together samples that are close together. The advantage in this is that objects that are close togther physically tend to be affected by the same lights, and therefore by picking samples that are close together we make sure that those samples sahre similar lights. So what we will do is subdivide our view space into clusters, first by performing a 2D subdivision in screen space and then subdividing even further in depth. How to exactly perform this depth subdivision is also up to the algorithm implementer but in the paper they briefly examine three possible ways to do it. 

<iframe id="zslice1" class ="slideshow-iframe" src="{{ site.baseurl }}/slides/zslices.html" style = "width:100%" frameborder="0" scrolling="no" ></iframe>

The animation above showcases these three possible subdivision schemes, subdivision in NDC, regular view frustum subdivision and exponential subdivision, in order of appearance. The first one presents issues because although the segments are uniform in NDC, NDC itself is rather non-uniform in view space. This results in very thin clusters close to the camera and very large clusters close to the far plane. The opposite occurs in uniform subdivision, where near clusters are long and narrow and those far awaya re very wide and flat. The next attempt consists in making them exponentially distributed in such a way that we counteract the non-linearity of transforming NDC space to view space. The paper then goes ahead and introduces their equation for non linear view space which you can find below:

$$\begin{equation} k = \left\lfloor\dfrac{ \log\left(-Z_{vs} / \text{near}\right)}{\log\left(1 +  \frac{2 tan \theta}{S_y}\right)}\right\rfloor \end{equation}$$ 

In my implementation I ended up using a logarithmic distribution based on the one talked about in the DOOM siggraph presentation which you can see here:

$$\begin{equation} \text{Z} = \text{near}_z \times \left(\frac{\text{Far}_z}{\text{Near}_z} \right)^{\text{slice} / \text{numSlices}} \end{equation}$$ 

We can solve find which specific slice a pixel is in from their given Z depth using 

$$\begin{equation} \text{slice} = \left\lfloor\ \log\left( Z\right)\frac{\text{numSlices}}{\log\left( \text{Far}_z / \text{Near}_z\right)} -  \frac{\text{numSlices} \times \log\left( \text{Near}_z \right)}{\log\left( \text{Far}_z / \text{Near}_z\right)}  \right\rfloor \end{equation}$$ 

Which I have also plotted in Desmos and you can play with in [ this link ](https://www.desmos.com/calculator/bf0g6n0hqp) or just take a look at in  the plot below.

<iframe id="zslice2" src="https://www.desmos.com/calculator/msnl9ob6tx?embed" style="width:100%" frameborder="0"></iframe>

Here you can check out what this depth slicing looks like in a scene like Sponza, taken directly from my engine. The colors represent the index representation in binary looping every 8 z depths. 

<div class="img">
	<img class="col gif" src="{{ site.baseurl }}/img/CG/clusters.png">
</div>

 
So how exactly do you calculate these z clusters? Well, you can find the specific implementation I used in the following [link](https://github.com/Angelo1211/HybridRenderingEngine/blob/master/assets/shaders/ComputeShaders/clusterShader.comp). It is written in a glsl compute shader, so it might seem like nonsense if you are not familiar with it but the general idea of the algorithm is to do the following: 

Also picking the tile size is kind of arbitrary, so just experiment to see what works best in your system although it probably will be a tile size that evenly divides the screen to avoid wasted threads.


{% highlight c# %}
//Each cluster has it's own thread ID in x, y and z and we dispatch one thread per cluster
for each cluster
    //Eye position is zero in view space
    const vec3 eyePos = vec3(0.0);

    //Per cluster variables
    uint tileSizePx = tileSizes[3];
    uint tileIndex  = gl_GlobalInvocationID;

    //Calculating the min and max point in screen space
    vec4 maxPoint_sS = vec4(vec2(gl_WorkGroupID.x + 1,
                                 gl_WorkGroupID.y + 1) * tileSizePx,
                                 -1.0, 1.0); // Top Right
    vec4 minPoint_sS = vec4(gl_WorkGroupID.xy * tileSizePx,
                            -1.0, 1.0); // Bottom left

    //Pass min and max to view space
    vec3 maxPoint_vS = screen2View(maxPoint_sS).xyz;
    vec3 minPoint_vS = screen2View(minPoint_sS).xyz;

    //Near and far values of the cluster in view space
    //We use equation (2) directly to obtain the tile vales
    float tileNear  = -zNear * pow(zFar/ zNear, gl_WorkGroupID.z/float(gl_NumWorkGroups.z));
    float tileFar   = -zNear * pow(zFar/ zNear, (gl_WorkGroupID.z + 1) /float(gl_NumWorkGroups.z));

    //Finding the 4 intersection points made from the maxPoint to the cluster near/far plane
    vec3 minPointNear = lineIntersectionToZPlane(eyePos, minPoint_vS, tileNear );
    vec3 minPointFar  = lineIntersectionToZPlane(eyePos, minPoint_vS, tileFar );
    vec3 maxPointNear = lineIntersectionToZPlane(eyePos, maxPoint_vS, tileNear );
    vec3 maxPointFar  = lineIntersectionToZPlane(eyePos, maxPoint_vS, tileFar );

    vec3 minPointAABB = min(min(minPointNear, minPointFar),min(maxPointNear, maxPointFar));
    vec3 maxPointAABB = max(max(minPointNear, minPointFar),max(maxPointNear, maxPointFar));

    //Saving the AABB at the tile linear index
    //Cluster is just a SSBO made of a struct of 2 vec4's
    cluster[tileIndex].minPoint  = vec4(minPointAABB , 0.0);
    cluster[tileIndex].maxPoint  = vec4(maxPointAABB , 0.0);
                                                                                                                
{% endhighlight %}

There are two functions in this process worth highlighting: the line intersection to plane and the screen2View function. The screen 2 view function converts a given point on screen space to view space by taking the reverse transformation steps of the graphics pipeline. We begin from Screen space, transsform to NDC then to clipspace and lastly to view space by using the inverse projection matrix and dividing by w. I found this function in [Matt Pettineo's Blog THE DANGER ZONE](https://mynameismjp.wordpress.com/2009/03/10/reconstructing-position-from-depth/), thanks Matt!

{% highlight c# %}
vec4 screen2View(vec4 screen){
    //Convert to NDC
    vec2 texCoord = screen.xy / screenDimensions.xy;

    //Convert to clipSpace
    vec4 clip = vec4(vec2(texCoord.x, 1.0 - texCoord.y)* 2.0 - 1.0, screen.z, screen.w);

    //View space transform
    vec4 view = inverseProjection * clip;

    //Perspective projection
    view = view / view.w;
    
    return view;
}
{% endhighlight %}

To obtain the points on the cluster we use a line intersection to plane function, since we already know the plane normal from the fact we are working in viewspace, we also know the the origin of the camera in viespace is zero. The algorithm is kind of self explanatory here:

{% highlight c# %}
  //Creates a line from the eye to the screenpoint, then finds its intersection
  //With a z oriented plane located at the given distance to the origin
  vec3 lineIntersectionToZPlane(vec3 A, vec3 B, float zDistance){
      //all clusters planes are aligned in the same z direction
      vec3 normal = vec3(0.0, 0.0, 1.0);
      //getting the line from the eye to the tile
      vec3 ab =  B - A;
      //Computing the intersection length for the line and the plane
      float t = (zDistance - dot(normal, A)) / dot(normal, ab);
      //Computing the actual xyz position of the point along the line
      vec3 result = A + t * ab;
      return result;
  }

{% endhighlight %}

Why do we use AABB's instead of tight frustums? Well, AABBS built from the max and min points will be ever so slightly larger than the actual clusters, this ensures that there are no gaps in between the clusters. Using AABB's also allows for storage of the cluster using only 2 vec3's, one for each corner of the AABB. Which reduces bandwidth significantly. Once we have AABB's for each cluster we are ready to use it in our per-frame light culling and active cluster determination schemes. These AABB's will be valid for as long as the frustum stays within this shape. Any changes in FOV or near and far plane settings will require a full frustum reconstruction, but I think it might not be a huge deal since RenderDoc seemed to tell me this function was not too expensive in terms of performance costs. It needs to actually be tested to see, maybe getting more clusters would screw up things not sure yet



### **Determining Active Clusters**
<p></p>

This part is rather optional, since we can just check all clusters for all lights for now. However, if you do go for this you will have to do a ZPre-pass, so this is only really worth it if you were doing this anyway for some reason. Now, I have not fully implemented this yet, it is planned for a future upgrade of my engine but a naive implementation of it in could be done like so:

{% highlight c# %}
//We will evaluate the whole screen in one compute shader 
//so each thread is equivalent to a pixel
void markActiveClusters(){
    vec2 screenCord = pixelID.xy / screenDimensions.xy;
    float z = texture(screenCord)
    uint index = getClusterIndex(pixelID.xy, z);
    clusterActive[index] = true;
}
{% endhighlight %}

With this compute shader we would mark all the active clusters in a grid simply by writing to an array of bools as large as the cluster grid and then we would jsut set the flag as true if it is active at any point. To increase efficiency both Van Oosten and Olsson compact this list into a set of unique clusters to check from. This helps because it reduces the amount of clusters that need to have lights culled against them, instead of having to cull every light to every cluster, which would make the problem O(lights * clusters), remember that this was originally O(meshes * lights) so by using a coarser world grid of clusters we significantly improve performance if clusters < meshes.


{% highlight c# %}
//We will every cluster in one compute shader
void buildCompactClusterList(){
    uint tileIndex  = gl_GlobalInvocationID;
    if(clusterActive[index]){
       uint offset = atomicAdd(globalActiveClusterCount, 1);
       uniqueActiveClusters[offset] = tileIndex;
    }
}
{% endhighlight %}

The global active cluster count variable keeps track of how many active clusters there are currently in the scene, and the unique active clusters array is a linear array with a size equal to the amount of clusters and will be rest to zero before calling this function. This way we now have an index of how many active clusters there are and we can launch computes in an indirect manner using this size. Once this is done we now have succesfully completed the active cluster and unique list of cluster determination step and we can move to light culling.

### **Light Culling Methods**
<p></p>

Explain the light culling step how you performed it

show some pseudocode

explain the benefits again of culling thisway

explain that it is essentially a spatial grouping of lights


maybe cute animation of the ball entering the sphere and getting added to the list of lights

### **Optimization Techniques**
<p></p>

Once you have a working implementation there is still much that can be done to improve performance, van oosten [link]() shows specifically how spatial optimization structures like BVH can significantly increase the light culling step to the point where up to a million lights can be used in real time performance. Olsson [link]() presents various methods that leverage the benefits of clustered shading, one of them focuses on making these many lights be shadow casting using virtual shadow mapping and another presents and alternate method of clustering using the normal at the surface instead of the world space position. 

There are actual applications for renderers that use that many lights. Specifically in the field of global illumination, where you can fake indirect lighting by using many small lights instead. Another improvement we can make with clustered rendering is to do what DOOM 2016 did and include many different things in the clusters, not just lights but also decals and environment probes. This opens up the possibility of using this very coarse voxel grid to perform any kind of world space voxelization algorithms. 

Also something that is metnioned in the papers is that the strength of clustered shading is that it allows to turn splatting techniques into gathering techniques. There is also a lot of talk these days about gpu level optimizations you can do at the wave level which I have not researched in depth yet but would like to soon at some time. I might go into this at some point in my project or not, I am not sure yet

<p></p>
### **Successful Implementations**
<p></p>

So I would be remiss not to end this post by sharing some great presentations from other way more experienced devs where they discuss their own versions of Clustered shading. As you can imagine, no two of them are alike, both in the nitty-gritty details of their implementations and in the large scale differences found in the genres of the games they make. It speaks volumes of the flexibility of this algorithm and hints towards a future where hybrid forward/deferred engines are the norm for AAA products that demand the highest visual fidelity. 

Also, hey, I've technically implemented a Clustered Renderer too! If you've forgotten about it after reading this whole post I don't blame you, but [here's a link](https://github.com/Angelo1211/HybridRenderingEngine) for you so you don't have to scroll all the way to the top. It's a simplified implementation with much of the code commented so you &mdash; and me in about a year when I forget &mdash; can follow along easily. Okay okay, enough self promotion, here are the goods:

* [Clustered Shading in the Wild by Ola Olsson](http://efficientshading.com/blog/)
* [(Doom 2016)The devil is in the Details by Tiago Sousa and Jean Geffrey](https://www.slideshare.net/TiagoAlexSousa/siggraph2016-the-devil-is-in-the-details-idtech-666?next_slideshow=1)
* [Practical Clustered Shading by Emil Persson](http://www.humus.name/Articles/PracticalClusteredShading.pdf)
* [Clustered Forward Rendering and Anti-aliasing in 'Detroit: Become Human' by Ronan Marchalot](https://twvideo01.ubm-us.net/o1/vault/gdc2018/presentations/Marchalot_Ronan_ClusteredRenderingAnd.pdf)

#### Further reading:
* [Chapter 20: Efficient Shading in Real Time Rendering 4th Edition](http://www.realtimerendering.com/)
* [Volume Tiled Forward Shading by Jeremiah Van Oosten](https://www.3dgep.com/volume-tiled-forward-shading/)
* [Practical Clustered Shading By Emil Persson](http://newq.net/dl/pub/SA2014Practical.pdf)
* [Tiled and Clustered Forward Shading Supporting Transparency and MSAA Olsson et al.](https://www.researchgate.net/publication/254463291_Tiled_and_Clustered_Forward_Shading_Supporting_Transparency_and_MSAA)

---

*Psst, hey you! If you've gotten this far I'd like to thank you for taking the time to read this post. I hope you enjoyed reading it as much as I did writing it. Just letting you know that I'm currently looking for a Graphics related position. If you're interested or know of any opportunities, I'd appreciate it if you'd let me know. Feel free to check out my about page for a choice selection of social media where you can reach me!*

*Any questions? Dead links? Complaints to management? My email is angelo12@vt.edu but you can also find me on twitter at [@aortizelguero](https://twitter.com/aortizelguero).*

